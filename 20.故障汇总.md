#### 1.etcd集群启动出现问题

```yaml
出现no host route
检查iptables和firewalld
```



#### 2.flannel部分节点未启动报错导致node节点未出现cni网卡

```yaml
主节点的flannel出现报错 
Failed to create SubnetManager: error  dial tcp 10.96.0.1:443: i/o timeout

访问apiserver出现超时，判断ipvs是否出现问题

ipvsadm -Ln发现没有任何ipvs规则

重新启动kube-proxy,生产ipvs规则

原因: kube-proxy启动先于apiserver，导致未从etcd中刷新数据
```



#### 3.所有被删除的pods一直处于Terming状态

```yaml
pods一直未回收,删除一直处于not ready的test244节点解决
kubectl delete node test244
```



#### 4.发现执行kubectl get pods

```yaml
Unable to connect to the server: EOF

检查apiserver都正常
检查haproxy正常
但是etcd集群被降级
etcd集群故障
```



#### 5.修改docker的驱动格式后重启，pods不在调度在本节点

```yaml
重启docker后，kubelet未发送心跳到apiserver，导致node处于not-ready状态
重启kubelet可以解决
```



#### 6.kubeadm重启后不会自动恢复，查看日志发现找不到bootstrap-kubelet.conf文件，添加该文件到指定目录解决

```yaml
Jun  8 16:15:06 master03 kubelet: F0608 16:15:06.837280   11867 server.go:265] failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory
```



#### 7.pod的大量标准输出的日志导致主机磁盘超过80%,驱离本节点的所有pod

```yaml
容器的日志 则可以通过 docker logs 命令来访问，而且可以像 tail -f 一样，使用 docker logs -f 来实时查看。如果使用 Docker Compose，则可以通过 docker-compose logs <服务名> 来查看。

如果深究其日志位置，每个容器的日志默认都会以 json-file 的格式存储于 /var/lib/docker/containers/<容器id>/<容器id>-json.log 下，不过并不建议去这里直接读取内容，因为 Docker 提供了更完善地日志收集方式 - Docker 日志收集驱动。

关于日志收集，Docker 内置了很多日志驱动，可以通过类似于 fluentd, syslog 这类服务收集日志。无论是 Docker 引擎，还是容器，都可以使用日志驱动。比如，如果打算用 fluentd 收集某个容器日志，可以这样启动容器：

设置Docker容器日志大小（治本）
1.  新建配置文件: /etc/docker/daemon.json，若有就不用新建了。(当标准输出日志大于100m时，将该文件清空并从0开始，但不影响外部hostpath挂载的日志文件)
{
"log-driver":"json-file",
"log-opts": { "max-size": "100m", "max-file": "1" }
}

2.重启docker守护进程 
systemctl daemon-reload
systemctl restart docker
```



#### 8.nginx-ingress权限问题

```yaml
E0717 05:40:12.244151       6 checker.go:41] healthcheck error: Get http+unix://nginx-status/healthz: dial unix /tmp/nginx-status-server.sock: connect: no such file or directory
I0717 05:40:12.247192       6 main.go:172] Received SIGTERM, shutting down
I0717 05:40:12.247238       6 nginx.go:387] Shutting down controller queues
I0717 05:40:12.247261       6 status.go:116] updating status of Ingress rules (remove)
I0717 05:40:12.258558       6 status.go:135] removing address from ingress status ([])
I0717 05:40:12.258631       6 nginx.go:395] Stopping NGINX process
nginx: [alert] could not open error log file: open() "/var/log/nginx/error.log" failed (13: Permission denied)
2020/07/17 05:40:12 [notice] 180#180: signal process started
2020/07/17 05:40:12 [error] 180#180: invalid PID number "" in "/tmp/nginx.pid"
I0717 05:40:12.265809       6 main.go:176] Error during shutdown: exit status 1
I0717 05:40:12.265836       6 main.go:180] Handled quit, awaiting Pod deletion
E0717 05:40:16.661673       6 checker.go:41] healthcheck error: Get http+unix://nginx-status/healthz: dial unix /tmp/nginx-status-server.sock: connect: no such file or directory
I0717 05:40:22.265977       6 main.go:183] Exiting with 1


chmod 777 /var/log/nginx-controller/
```

